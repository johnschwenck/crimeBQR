---
title: "Methods"
author: "Marshall Honaker"
output: pdf_document
urlcolor: blue
---
First, let's import the necessary packages:
```{r}
library(fields)
library(RandomFields)
library(mvtnorm)
library(ggplot2)
library(matrixcalc)
```


Wu and Narisetty (2020) outline a score based likelihood approach to Bayesian multiple quantile regression. Here we provide R code for a modified version of this approach adapted for spatial data. Annotations will be made/provided to explain the ways in which the approach taken by Wu & Narisetty was modified to accomdodate spatially dependent data. 

To begin, we will define so of the more basic, necessary functions. First, we will define the score function. 

# Basic Functions & Methods
In the aforementioned paper, the score function is given by: $s_\tau (\beta) = \sum_{i = 1}^n x_i \psi_\tau (y_i - x_i^T \beta)$, where $\psi_\tau(u) = \tau - I_{\{ u < 0 \}}(u)$. 
```{r}
get_score<- function(y, X, tau, beta){ 
  n <- nrow(X)
  temp <- t(X[1,,drop=FALSE])*ifelse(y[1] - X[1,,drop=FALSE]%*%as.matrix(beta) < 0, tau  - 1, tau)[1,1]
  for(i in 2:n){
    temp <- temp + t(X[i,,drop=FALSE])*ifelse(y[i] - X[i,,drop=FALSE]%*%as.matrix(beta) < 0, tau  - 1, tau)[1,1]
  }
  return(temp)
}
```

I'm also going to define a function to obtain the spatial covaraince matrix. This isn't necessary, but it will make the code later on more readable.
```{r}
get_spatial_covar_mat <- function(locs){  ## outputs nxn spatial covariance matrix
  dist <- rdist(locs)
  return(exp(-dist/1.5))
}
```

Next, we will write a method to obtain a spatially adapted version of the working likelihood function proposed in the paper. The function proposed in the paper is given by $L(Y | X, \beta) = C \exp\left( -\frac{1}{2n} s_\tau(\beta)^T W s_\tau(\beta) \right)$ where $W$ is a p x p positive definite weight matrix. To account for the spatial variability/dependence within the data, we will instead use following quantity inspired by the Mahalanobis Distance: $(X - \hat{\mu})^T \Sigma^{-1} (X - \hat{\mu})$, where $\Sigma^{-1}$ is the inverse of the spatial covariance matrix. 

However, it is important to note that while the weight matrix proposed by the paper is positive definite, the quantity we use to replace it is only positive semi-definite. We have also omitted the exponentiation of the kernel of the likelihood function. (This yields sensible results when working with the Importance Sampling algorithm defined later on, but yields extremely large values which is very strange for a likelihood function.)
```{r}
# X: original design matrix with first two columns corresponding to coordinates
# b: # of bases --> default to 20 *
# k: # of knots
X_combined <- function(X, b, k, locs){ 
  
  coords <- locs
  B = bs(1:nrow(X), df = b) # use B-splines using n of X and df = # of bases --> dim: n x b matrix

  spline_smooth <- eval_basis(B, coords) # fix... n x "basis" smooth matrix <-- (n x b) & (n x 2)
  X_basis <- cbind(X, spline_smooth) # n x (p + "basis") <-- n x p original feature matrix "+" n x "basis" smooth matrix (bind not add)
  
  return(X_basis) #n x (p + "basis")
}

# X = X_basis from  X_combined function
weight <- function(X, tau){
  n <- nrow(X)
  temp <- X[1,]%*%t(X[1,])
  for(i in 2:n){
    temp <- temp + X[i,]%*%t(X[i,])
  }
  coef <- n/(tau*(1 - tau))
  return(coef*solve(temp))
}

get_likelihood <- function(y, X, tau, beta, locs, C){ 
  ## Outputs 1x1 scalar that is the likelihood
  score <- get_score(y, X, tau, beta)
  #X_centered <- X - colMeans(X)
  coef <- -1/(2*length(y))
  weight_X <- weight() # fix...
  kernel <- exp(coef*(t(score)%*%weight_X%*%score)[1,1])
  return(C*kernel)
}
```

## Test Basic Methods
Now that we have defined the basic functions, let's test them to make sure they work.
```{r}
response <- rnorm(500)
feature_mat <- cbind(rnorm(500), rnorm(500))
t <- .5
B <- c(1, 1)
l <- cbind(runif(500, 0, 10), runif(500, 0, 10))

get_score(response, feature_mat, t, B)
get_likelihood(response, feature_mat, t, B, l, 1)
```

Everything seems to be working alright but we should keep an eye on the likelihood function. It seems to work for now, but the magnitude of the values produced is very strange for a likelihood function.  

# Ada Importance Sampling for a Single Quantile Level
To approximate the posterior distribution of the model parameters, Wu & Narisetty implement an Importance Sampling (IS) procedure described on pgs. 13-16. The following code implements this IS algorithm. It runs, but we still need to check the accuracy and efficiency of the results. 

```{r}
get_updated_params <- function(y, X, tau, beta, Sigma, locs, draw){  
  ## outputs list with elements mu (a p-length vector of estimated means) and S (the estimated pxp covariance matrix of the features) calculated from the simulated values given in draw. This performs Step 3 outlined in the paper
  n <- length(y)
  p <- ncol(X)
  
  w <- get_likelihood(y, X, tau, beta, locs, 1)*(1/(2*n)^p)/dmvnorm(draw, beta, Sigma)
  
  mu_hat <- apply(w*draw, 2, sum)/sum(w)
  
  S <- matrix(nrow = p, ncol = p)
  for(i in 1:p){
    for(j in 1:p){
      S[i,j] <- sum(w*draw[,i]*draw[,j])/sum(w) - mu_hat[i]*mu_hat[j]
    }
  }
  temp <- list(mu_hat, S)
  names(temp) <- c("mu", "S")
  return(temp)
}

get_effective_samp_size <- function(y, X, tau, beta, Sigma, locs, draw, M){
  ## Obtains Effective Sample Size
  p <- ncol(X)
  n <- length(y)
  w <- get_likelihood(y, X, tau, beta, locs, 1)*(1/(2*n)^p)/dmvnorm(draw, beta, Sigma)
  cv_sq <- ((M - 1)^(-1))*sum((w - mean(w))^2)/mean(w)^2
  return(M/(1 + cv_sq))
}

adaIS_singleQuantile <- function(y, X, tau, C, locs, M, num_reps){ 
  ## outputs a list with $mu (a p-length vector of estimated means) and $S (the estimated pxp covariance matrix of the features); the posterior mean vector and covariance matrix for the posterior distribution of the model parameters. This method runs the entire adaIS algorithm for a single quantile outlined in the paper
  p <- ncol(X) ## Step 1: Initialize starting values for IS algorithm
  n <- length(y) ## Step 1: Initialize starting values for IS algorithm
  beta <- coef(lm(y ~ X))[-1] ## Step 1: Initialize starting values for IS algorithm
  S <- cov(X) ## Step 1: Initialize starting values for IS algorithm
  
  params <- list(beta, S)  ## Put our initial parameter estimates/starting values into a list called params so we can update it in the loop and use the same name convention
  names(params) <- c("mu", "S")
  
  for(i in 1:num_reps){ ## Step 4: Repeat steps 2 and 3 until we achieve the desired effective sample size (the effective sample size has not been coded yet, but I will write that up next)
    draws <- rmvnorm(M, params$mu, params$S)  ## Step 2: Simulate M values from the proposal distribution
    params <- get_updated_params(y, X, tau, params$mu, params$S, locs, draws) ## Step 3: Update the parameter values using the simulated values from Step 2 and the importance weights 
  }
  return(params)
}
```

## Test IS Algorithm
```{r}
m <- 10000
S_0 <- cov(feature_mat)
initial_draw <- rmvnorm(m, B, S_0)

param_test <- get_updated_params(response, feature_mat, t, B, S_0, l, initial_draw)
param_test  ## Check updated parameter values
is.positive.definite(round(param_test$S, 7)) ## Make sure updated covariance matrix is positive definite

posterior_param_test <- adaIS_singleQuantile(response, feature_mat, t, 1, l, m, 10)
posterior_param_test ## Check posterior parameter values
is.positive.definite(round(posterior_param_test$S, 7)) ## Make sure posterior covaraince matrix is positive definite

get_effective_samp_size(response, feature_mat, t, B, cov(feature_mat), l, initial_draw, m)
```

So everything seems to be working, though it does need a little tuning/refinement. 

# Simulation Study
Now let's put everything together to conduct a more thorough simulation study.
```{r}
## First, let's simulate the locations
n <- 1000
locs <- as.data.frame(cbind(runif(n, 0, 100), runif(n, 0, 100)))
colnames(locs) <- c("long", "lat")

## Next, we will simulate the spatially dependent values
sim_mod <- RMexp(var = 4, scale = 3) + 
  RMnugget(var = .75) + 
  RMtrend(mean = 1.5)
sim_vals <- RFsimulate(sim_mod, x = locs$long, y = locs$lat)

## Now, we will simulate the values of the response and the covariates 
beta <- c(2, 2, 5, 5, 5, 5)
X_sim <- cbind(rnorm(n, 10, 2), rexp(n, 5), rbeta(n, 5, 1), runif(n, 2.5, 7.5))
y_sim <- beta[1]*locs$long + beta[2]*locs$lat + 
  beta[3]*X_sim[,1] + beta[4]*X_sim[,2] + beta[5]*X_sim[,3] + beta[6]*X_sim[,4]

## Next, we will split the data into training and test sets (75% of simulated values will be allocated to the training data set and the remaining 25% of simulated values will be used as test data)
train_indices <- sample(1:n, round(.75*n), replace = FALSE)
X_train <- X_sim[train_indices,]
X_test <- X_sim[-train_indices,]
y_train <- y_sim[train_indices]
y_test <- y_sim[-train_indices]
locs_train <- locs[train_indices,]
locs_test <- locs[-train_indices,]

## Now, to plot the training data
sim_data <- as.data.frame(cbind(y_train, locs_train))
sim_plot <- ggplot(data = sim_data, aes(x = long, y = lat)) + geom_point(aes(col = y_train, size = y_train)) +
  labs(title = "Plot of Simulated Values at Simulated Locations",
       x = "Longitude",
       y = "Latitude",
       col = "Response",
       size = "Response") +
  scale_color_gradient(low = "blue", high = "red", name = "Response")
plot(sim_plot)

## Now that we have the simulated data, we can check out the performance of our functions for a variety of quantile levels
tau <- c(.01, .05, .1, .25, .5, .75, .9, .95, .99)
b <- beta[3:length(beta)]

score_mat <- matrix(nrow = length(b), ncol = length(tau))
for(i in 1:ncol(score_mat)){
  score_mat[,i] <- get_score(y_train, X_train, tau[i], b)
}
score_mat

likelihood_vec <- rep(NA, length(tau)) 
for(i in 1:length(tau)){
  likelihood_vec[i] <- get_likelihood(y_train, X_train, tau[i], b, locs_train, 1)
}
likelihood_vec

draw <- rmvnorm(n, b, cov(X_train))
mu_mat <- matrix(nrow = length(b), ncol = length(tau))
S_list <- list()
param_list <- list(mu_mat, S_list)
names(param_list) <- c("mu", "S")
for(i in 1:length(tau)){
  x <- get_updated_params(y_train, X_train, tau[i], b, cov(X_train), locs_train, draw)
  param_list$mu[,i] <- x$mu
  param_list$S[[i]] <- x$S
  draw <- rmvnorm(n, x$mu, x$S)
}
param_list

M <- 10000
draw <- rmvnorm(n, b, cov(X_train))
num_iters <- 10
post_mu_mat <- matrix(nrow = length(b), ncol = length(tau))
post_S_list <- list()
post_ess <- rep(NA, length(tau))
post_param_list <- list(post_mu_mat, post_S_list, post_ess)
names(post_param_list) <- c("mu", "S", "ESS")
for(i in 1:length(tau)){
  temp <- adaIS_singleQuantile(y_train, X_train, tau[i], 1, locs_train, M, num_iters)
  post_param_list$mu[,i] <- temp$mu
  post_param_list$S[[i]] <- temp $S
  post_param_list$ESS[i] <- get_effective_samp_size(y_train, X_train, tau[i], b, cov(X_train), locs_train, draw, M)
}
post_param_list


```


# Issues so Far

In developing this novel approach to Bayesian spatial quantile regression, we have encountered several issues. We will list them here.

* **Likelihood Function**: The working likelihood function in the original paper specifies a positive definite weight matrix $W$. To accommodate the spatial dependencies in our data, we instead used a spatially adjusted quantity inspired by the Mahalanobis Distance: $(X - \hat{\mu})^T \Sigma^{-1} (X - \hat{\mu})$, where $\Sigma^{-1}$ is the inverse of the spatially varying covariance matrix. 
  + However, the quantity used to replace the sample Mahalanobis Distance is not positive definite (it is only positive semi-definite). The methods seem to work for the time being, but moving forward we should clarify what role this difference plays in the function and performance of the model.
  + Furthermore, the spatially adapted Mahalanobis distance produces extremely large quantities. In the original function, the quadratic form of the score function and the weight matrix was exponentiated with a negative coefficient (that is, the kernel of the likelihood function was given by: $\exp\left( -\frac{1}{2n} s_\tau (\beta)^T W s_\tau (\beta) \right)$. However, when we replace this with the extremely large quantities generated by our spatially adapted Mahalanobis distance, the likelihood becomes effectively zero. This wreaks havoc with the rest of the code (since a likelihood of zero makes it difficult to do any calculations whatsoever) so we removed the exponentiation and the negative sign (on the coefficient) from the likelihood. This seems to yield reasonable posterior parameter estiamtes, but we will need to do some more testing to make sure that these posterior predictions are in fact accurate.
  + How can we choose a value for $C$ in the likelihood function? I've been using 1 so far, since that will let the code run nicely for the simulation, but when we apply it to the actual data, how can we select a particular value? 
  + Lastly, I think this approach makes sense, but I'd like to make sure that it isn't biased or will produce misleading results. (By using the spatial precision matrix instead of the precision matrix of the features, we are essentially scaling the feature centered observations using the )
* **Evaluation of Simulations**: 
  + In the paper, the authors mostly compare the performance of the IS algorithm to other well known MCMCs across various quantiles levels. We might be able to do this, but it could be difficult since there aren't many papers or packages that deal specifically with Bayesian spatial quantile regression. Instead, I might suggest comparing the results from our model/approach to the model/approach taken by the select few other papers that deal with this subject. (Namely, [Reich et al. (2011)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3583387/), [Ramsey (2019)](https://onlinelibrary-wiley-com.srv-proxy1.library.tamu.edu/doi/abs/10.1093/ajae/aaz029), and/or [King & Song (2016)](https://www.tandfonline.com/doi/abs/10.1080/02664763.2018.1508557?journalCode=cjas20)).
  + Also, since this is a Bayesian model, it's a little trickier to makes predictions. (Since the model parameters are themselves random variables we can't just plug in the observations to get out predictions.) We will need to derive a posterior predictive distribution to do this. (I will do my best to derive this and code it as soon as possible.)
* **"Score" Function**: The likelihood function relies on a "score" function given by: $s_\tau (\beta) = \sum_{i = 1}^n x_i \psi_\tau \left( y_i - x_i^T \beta \right)$. It should be noted that this is not the traditional definition of the "score" function commonly used in inference and likelihood based statistics, but instead the "rankscore" function, which is simply a more functional way of formulating regression quantiles. (Please see slide 15 of [Koenker (2003)](http://www.econ.uiuc.edu/~roger/NAKE/wk2.pdf) and [Guttenbrunner and Jurečková (1992)](https://projecteuclid.org/download/pdf_1/euclid.aos/1176348524)).
